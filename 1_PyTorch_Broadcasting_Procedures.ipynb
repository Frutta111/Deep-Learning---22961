{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1z/xdAo6mdf/n7WGYc44D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frutta111/Deep-Learning-In-PyTorch/blob/main/1_PyTorch_Broadcasting_Procedures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3/4/2024\n"
      ],
      "metadata": {
        "id": "KVW1-oRmI08D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implemention the broadcast functionality of PyTorch**\n",
        "\n",
        "Welcome to this notebook on PyTorch and working with tensors.\n",
        "\n",
        "In this session, we will explore the broadcast functionality.\n",
        "\n",
        "`Broadcasting` is a method that allows PyTorch to perform element-wise operations on tensors of different shapes by automatically expanding their dimensions to match each other. It is needed to simplify and optimize operations on tensors, making code more concise and efficient without requiring manual reshaping.\n",
        "\n",
        "Specifically, we will get into three important methods:\n",
        "\n",
        "1.  Implement the [`A.expand_as(B)`](https://pytorch.org/docs/stable/generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as) method.\n",
        "2.  Determine if two tensors can be broadcasted together.\n",
        "3.  Implement the [`torch.broadcast_tensors(A,B)`](https://pytorch.org/docs/stable/generated/torch.broadcast_tensors.html) method.\n",
        "\n",
        "For all these tasks, we will avoid using PyTorch's built-in functions/methods that perform broadcasting. Instead, we will build our implementations from scratch to gain a deeper understanding of how these operations work.\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "uIPGOcKU-WN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mGStv8e0I0QL"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. expand_as method**"
      ],
      "metadata": {
        "id": "2EqpMC3OZ5kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We follow the General Broadcasting Rules and carry out the implementation step by step:\n",
        "\n",
        "First, for tensors of different dimensions, check if they are compatible:\n",
        "Start from the last dimension (rightmost) of both tensors and check,\n",
        "\n",
        "   * If they are equal to each other, or\n",
        "   * One of them is equal to 1.\n",
        "\n",
        "If none of these conditions holds true, an error will occur, move one dimension to the left on each tensor and repeat the above check."
      ],
      "metadata": {
        "id": "rD5KQDzzOoY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expandability_check(tensorA, tensorB) :\n",
        "\n",
        "  '''\n",
        "  Function to check if tensorA can be expanded to the shape of tensorB\n",
        "\n",
        "  Args:\n",
        "      tensorA (torch.Tensor): The tensor to be expanded.\n",
        "      tensorB (torch.Tensor): The tensor whose shape is to be matched.\n",
        "\n",
        "  Returns:\n",
        "      bool: True if tensorA can be expanded to the shape of tensorB, False otherwise.\n",
        "  '''\n",
        "\n",
        "  sizeA = tensorA.size() # Get the size of tensorA\n",
        "  sizeB = tensorB.size() # Get the size of tensorB\n",
        "\n",
        "  dimA = len(sizeA) # Get the number of dimensions of tensorA\n",
        "  dimB = len(sizeB) # Get the number of dimensions of tensorB\n",
        "\n",
        "  # Ensure tensorB has at least as many dimensions as tensorA\n",
        "  if dimB < dimA :\n",
        "    print(\"Error: the number of size provided must be greater or equal to the number of dimensions in tensor A\")\n",
        "    return False\n",
        "    # raise RuntimeError(\"The number of size provided must be greater or equal to the number of dimensions in tensor A\")\n",
        "\n",
        "  i = 1\n",
        "\n",
        "  # Check each dimension from the end to the beginning\n",
        "  while i <= dimA:\n",
        "    if sizeA[-i] != sizeB[-i] and sizeA[-i] != 1:\n",
        "      print((f\"The expanded size of the tensor ({sizeB[-i]}) must match the existing size ({sizeA[-i]}) at non-singleton dimension {dimB-i}. Target sizes: {sizeB}.  Tensor sizes: {sizeA}\"))\n",
        "      return False\n",
        "      # raise RuntimeError((f\"The expanded size of the tensor ({sizeB[-i]}) must match the existing size ({sizeA[-i]}) at non-singleton dimension {dimB-i}. Target sizes: {sizeB}.  Tensor sizes: {sizeA}\"))\n",
        "    i += 1\n",
        "\n",
        "  return True"
      ],
      "metadata": {
        "id": "3hkApsyGJ78t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, once we have traversed through all dimensions of at least one tensor from right to left and no error occurred, we can proceed to the broadcasting stage. During the broadcasting stage, the tensor values are multiplied according to the following rules:\n",
        "1. If one tensor has fewer dimensions than the other (i.e., smaller size), pad it at the beginning with 1s until the number of dimensions in both tensors is equal.\n",
        "2. In case of any disagreement in dimensionality, the compatibility check for broadcasting is successfully completed, hence one tensor has a dimension of 1, which is then duplicated along this dimension."
      ],
      "metadata": {
        "id": "emZuOXln17FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def broadcasting (tensorA, sizeB) :\n",
        "  '''\n",
        "  Function to broadcast tensorA to the shape of sizeB\n",
        "\n",
        "  Args:\n",
        "      tensorA (torch.Tensor): The tensor to be broadcast.\n",
        "      sizeB (torch.Size): The target shape for broadcasting.\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor: The broadcasted tensorA.\n",
        "  '''\n",
        "\n",
        "  sizeA = tensorA.size() # Get the size of tensorA\n",
        "\n",
        "  dimA = len(sizeA) # Get the number of dimensions of tensorA\n",
        "  dimB = len(sizeB)  # Get the number of dimensions of tensorB\n",
        "\n",
        "  tensorC = tensorA.clone()\n",
        "  dimC = dimA\n",
        "\n",
        "  # Padding degenerate dimensions on the left of tensor A until its dimension equals tensor B's dimension\n",
        "\n",
        "  while dimC < dimB :\n",
        "    tensorC.unsqueeze_(0) # Add a dimension of size 1 at the beginning\n",
        "    dimC += 1\n",
        "\n",
        "  sizeC = tensorC.size() # Get the size of tensorC after padding\n",
        "\n",
        "  # Expanding degenerate dimensions by # replicate tensorC along dimension i to match sizeB[i]\n",
        "  for i  in range(dimC) :\n",
        "    if sizeC[i] == 1:\n",
        "      tensorC = torch.cat([tensorC] * sizeB[i], dim=i)  #Concatenates the given sequence in the given dimension.\n",
        "    else :\n",
        "      continue\n",
        "\n",
        "  return tensorC"
      ],
      "metadata": {
        "id": "ed3De0sIMXgW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we can combime the procedures above to one for getting the implementation of our version to `expand_as`:\n"
      ],
      "metadata": {
        "id": "7kR_C1o3JA7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_expand_as(tensorA, tensorB) :\n",
        "  '''\n",
        "  Function to expand tensorA to the size of tensorB using the custom expandability_check and broadcasting functions\n",
        "\n",
        "  Args:\n",
        "      tensorA (torch.Tensor): The tensor to be expanded.\n",
        "      tensorB (torch.Tensor): The tensor whose size is to be matched.\n",
        "\n",
        "  Returns:\n",
        "      torch.Tensor: The expanded tensorA.\n",
        "  '''\n",
        "  # Check if tensorA can be expanded to the shape of tensorB\n",
        "  if expandability_check(tensorA, tensorB) :\n",
        "    return broadcasting(tensorA, tensorB.size())"
      ],
      "metadata": {
        "id": "8w42vNso2nUt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see some usage examples and check if our implementation  `my_expand_as` , is correct by comparing it with PyTorch's built-in `expand_as` method"
      ],
      "metadata": {
        "id": "Y2Goqv0y8_aX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1:  case when where A is expandable to B's dimensions"
      ],
      "metadata": {
        "id": "OpaqwX3pX1a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorA = torch.randint(10 , size=[4])\n",
        "tensorB = torch.arange(2*3*4).reshape(2,3,4)\n",
        "print(tensorA , tensorA.size())\n",
        "print(tensorB , tensorB.size())\n",
        "\n",
        "tensorC = my_expand_as(tensorA, tensorB)\n",
        "tensorD = tensorA.expand_as(tensorB)\n",
        "\n",
        "# Check if the custom and built-in \"expend as\" are equal\n",
        "if torch.equal(tensorC,tensorD):\n",
        "    print(\"\\nmy_expand_as matches Torch's expand_as:\")\n",
        "    print(tensorC , tensorC.size())\n",
        "else:\n",
        "    print(\"my_expand_as doesn't matches Torch's expand_as\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOTqJqQiWvxR",
        "outputId": "fe194b95-5b62-433f-da84-d525f00d773b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6, 7, 9, 7]) torch.Size([4])\n",
            "tensor([[[ 0,  1,  2,  3],\n",
            "         [ 4,  5,  6,  7],\n",
            "         [ 8,  9, 10, 11]],\n",
            "\n",
            "        [[12, 13, 14, 15],\n",
            "         [16, 17, 18, 19],\n",
            "         [20, 21, 22, 23]]]) torch.Size([2, 3, 4])\n",
            "\n",
            "my_expand_as matches Torch's expand_as:\n",
            "tensor([[[6, 7, 9, 7],\n",
            "         [6, 7, 9, 7],\n",
            "         [6, 7, 9, 7]],\n",
            "\n",
            "        [[6, 7, 9, 7],\n",
            "         [6, 7, 9, 7],\n",
            "         [6, 7, 9, 7]]]) torch.Size([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2:  case when where A can not be expandabled to B's dimensions"
      ],
      "metadata": {
        "id": "3cWPiTksYZXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorA = torch.randint(10 , size=[2,2,4])\n",
        "tensorB = torch.arange(3*4).reshape(3,4)\n",
        "\n",
        "print(\"my_expand_as results:\")\n",
        "tensorC = my_expand_as(tensorA, tensorB)\n",
        "\n",
        "print(\"torch expand_as results:\")\n",
        "try:\n",
        "  tensorD = tensorA.expand_as(tensorB)\n",
        "except RuntimeError as e:\n",
        "  print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KEGwPHOYfbs",
        "outputId": "ebe2e926-347d-4e4f-e9b4-10c475ca188f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_expand_as results:\n",
            "Error: the number of size provided must be greater or equal to the number of dimensions in tensor A\n",
            "torch expand_as results:\n",
            "expand(torch.LongTensor{[2, 2, 4]}, size=[3, 4]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 3:  case when where A can not be expandabled to B's dimensions"
      ],
      "metadata": {
        "id": "bmBRqCg7jSeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorA = torch.ones(size=[1,3])\n",
        "tensorB = torch.arange(3).reshape(3,1)\n",
        "\n",
        "print(\"my_expand_as results:\")\n",
        "tensorC = my_expand_as(tensorA, tensorB)\n",
        "\n",
        "print(\"torch expand_as results:\")\n",
        "try:\n",
        "  tensorD = tensorA.expand_as(tensorB)\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bys_78sXn2rn",
        "outputId": "fc21f12f-d042-42e3-99d4-9d439e41a93d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_expand_as results:\n",
            "The expanded size of the tensor (1) must match the existing size (3) at non-singleton dimension 1. Target sizes: torch.Size([3, 1]).  Tensor sizes: torch.Size([1, 3])\n",
            "torch expand_as results:\n",
            "The expanded size of the tensor (1) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [3, 1].  Tensor sizes: [1, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Determine if two tensors can be broadcasted together**"
      ],
      "metadata": {
        "id": "U2u_6iBVqc--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are cases where tensors cannot be expanded directly but can be broadcasted together.\n",
        "\n",
        "We should first verify if broadcasting is possible before comparing the results.\n",
        "\n",
        "\n",
        "The `tensors_expandability_check` function determines if two tensors can be broadcasted together and computes the shape of the resulting tensor if broadcasting is possible.\n",
        "\n",
        "It accepts two tensors as input, and checks if they can be broadcasted together, according to broadcasting rules.\n",
        "The function will return a True/False value as well as additional output if they are broadcastable together: The size to which the tensors will be expanded to."
      ],
      "metadata": {
        "id": "Nh1DHuCFlV-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensors_expandability_check(tensorA, tensorB) :\n",
        "  \"\"\"\n",
        "  Check if two tensors can be broadcast together and determine the shape of the resulting tensor.\n",
        "\n",
        "  Args:\n",
        "        tensorA (torch.Tensor): The first tensor.\n",
        "        tensorB (torch.Tensor): The second tensor.\n",
        "\n",
        "  Returns:\n",
        "        tuple: A tuple where the first element is a boolean indicating if broadcasting is possible,\n",
        "               and the second element is the shape of the resulting tensor if broadcasting is possible.\n",
        "  \"\"\"\n",
        "  sizeA = tensorA.size() # Get the size of tensorA\n",
        "  sizeB = tensorB.size() # Get the size of tensorB\n",
        "\n",
        "  dimA = len(sizeA) # Get the number of dimensions of tensorA\n",
        "  dimB = len(sizeB) # Get the number of dimensions of tensorB\n",
        "\n",
        "  # Initialize shape to be the dimension of the larger tensor (in terms of dimensions)\n",
        "  if dimA < dimB :\n",
        "    shape = list(sizeB)\n",
        "  else :\n",
        "    shape = list(sizeA)\n",
        "\n",
        "  # Compare the dimensions of the tensors and update shape according to the broadcast rules or return False\n",
        "  i = 1\n",
        "  while i <= min(dimA,dimB):\n",
        "    if sizeA[-i] != sizeB[-i] and min(sizeA[-i], sizeB[-i]) != 1:\n",
        "      print((f\"The size of tensor a ({sizeA[-i]}) must match the size of tensor b ({sizeB[-i]}) at non-singleton dimension {len(shape)-i}\"))\n",
        "      return False\n",
        "    shape[-i] = (max(sizeA[-i], sizeB[-i]))\n",
        "    i += 1\n",
        "\n",
        "  # Return True and the shape of the mutual broadcat\n",
        "  return True, shape\n"
      ],
      "metadata": {
        "id": "aecvEFlQ84U1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see some usage examples and check if our implementation  `my_broadcast_tensors` is correct"
      ],
      "metadata": {
        "id": "OME5NoXenziA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1: case when A and B can be broadcasted together"
      ],
      "metadata": {
        "id": "jB5jxpQYdGSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorA = torch.randint(10 , size=[1,3])\n",
        "tensorB = torch.arange(3).reshape(3,1)\n",
        "\n",
        "print(\"tensors_expandability_check results:\")\n",
        "print(tensors_expandability_check(tensorA, tensorB))\n",
        "\n",
        "print(\"\\ntorch.broadcast_tensors results:\")\n",
        "tensorA1 , tensorB1 = torch.broadcast_tensors(tensorA, tensorB)\n",
        "print(tensorA1.size(), tensorB1.size() , sep = \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrN3eYn-dY6d",
        "outputId": "983e3706-7b33-4e18-ec35-13f443709f6b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensors_expandability_check results:\n",
            "(True, [3, 3])\n",
            "\n",
            "torch.broadcast_tensors results:\n",
            "torch.Size([3, 3])\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2: case when A and B can be broadcasted together"
      ],
      "metadata": {
        "id": "GF7q5n30dF8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorA = torch.randint(10 , size=[2,1,4,7])\n",
        "tensorB = torch.arange(5*2*3*1*7).reshape(5,2,3,1,7)\n",
        "\n",
        "print(\"tensors_expandability_check results:\")\n",
        "print(tensors_expandability_check(tensorA, tensorB))\n",
        "\n",
        "print(\"\\ntorch.broadcast_tensors results:\")\n",
        "tensorA1 , tensorB1 = torch.broadcast_tensors(tensorA, tensorB)\n",
        "print(tensorA1.size(), tensorB1.size() , sep = \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EW2H8_odZbI",
        "outputId": "3731c1be-5716-403f-82c4-ea8433499f22"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensors_expandability_check results:\n",
            "(True, [5, 2, 3, 4, 7])\n",
            "\n",
            "torch.broadcast_tensors results:\n",
            "torch.Size([5, 2, 3, 4, 7])\n",
            "torch.Size([5, 2, 3, 4, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 3: case when A and B cannot be broadcasted together"
      ],
      "metadata": {
        "id": "fMXUG51ZdFJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorA = torch.randint(10 , size=[2,1,4,5])\n",
        "tensorB = torch.arange(5*2*3*1*7).reshape(5,2,3,1,7)\n",
        "\n",
        "print(\"tensors_expandability_check results:\")\n",
        "print(tensors_expandability_check(tensorA, tensorB))\n",
        "\n",
        "print(\"\\ntorch.broadcast_tensors results:\")\n",
        "try:\n",
        "  tensorA1 , tensorB1 = torch.broadcast_tensors(tensorA, tensorB)\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiPztNgIdZxT",
        "outputId": "04e28574-b01d-4893-b09c-c88babb35917"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensors_expandability_check results:\n",
            "The size of tensor a (5) must match the size of tensor b (7) at non-singleton dimension 4\n",
            "False\n",
            "\n",
            "torch.broadcast_tensors results:\n",
            "The size of tensor a (5) must match the size of tensor b (7) at non-singleton dimension 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 4: case when A and B cannot be broadcasted together"
      ],
      "metadata": {
        "id": "65kqlNZnf7sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorA = torch.randint(10 , size=[8,3,3,2,1,4,7])\n",
        "tensorB = torch.arange(5*2*3*4*7).reshape(5,2,3,4,7)\n",
        "\n",
        "print(\"tensors_expandability_check results:\")\n",
        "print(tensors_expandability_check(tensorA, tensorB))\n",
        "\n",
        "print(\"\\ntorch.broadcast_tensors results:\")\n",
        "try:\n",
        "  tensorA1 , tensorB1 = torch.broadcast_tensors(tensorA, tensorB)\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQmO3jsjkDlG",
        "outputId": "fa6ec721-55bf-455e-a39e-dcd5e4bd01c9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensors_expandability_check results:\n",
            "The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 2\n",
            "False\n",
            "\n",
            "torch.broadcast_tensors results:\n",
            "The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. torch.broadcast_tensors**\n",
        "\n",
        "The function `my_broadcast_tensors` - receives as input two tensors, and broadcasts them together. The function will return as output two new tensors\n",
        "\n",
        "\n",
        "*   It first checks if the tensors can be broadcasted together using the `tensors_expandability_check` function (defined at part 2).\n",
        "*   If the tensors can be broadcasted (`tensors_expandability_check` returns True), it retrieves the shape for broadcasting.\n",
        "*   It then broadcasts both tensors A and B to the common shape using a  `broadcasting` function (defined at part 1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A5fT3reeuQLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_broadcast_tensors(tensorA, tensorB) :\n",
        "  \"\"\"\n",
        "   Broadcast two tensors to a common shape if they are broadcast-compatible.\n",
        "\n",
        "   Args:\n",
        "        tensorA (torch.Tensor): The first tensor.\n",
        "        tensorB (torch.Tensor): The second tensor.\n",
        "\n",
        "   Returns:\n",
        "        tuple: A tuple of two tensors, each broadcasted to the common shape,\n",
        "               or None if the tensors cannot be broadcast together.\n",
        "  \"\"\"\n",
        "  # Check if tensors A and B can be broadcasted together\n",
        "  if tensors_expandability_check(tensorA, tensorB) :\n",
        "    # If tensors are expandable, get the shape for broadcasting\n",
        "    check, shape = tensors_expandability_check(tensorA, tensorB)\n",
        "    # Broadcast tensors A and B to the common shape and return the broadcasted tensors\n",
        "    return broadcasting(tensorA, shape) , broadcasting(tensorB, shape)\n"
      ],
      "metadata": {
        "id": "KGRDfAG0tzkn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see some usage examples and check if our implementation  `my_broadcast_tensors` , is correct by comparing it with PyTorch's built-in `torch.broadcast_tensors` method"
      ],
      "metadata": {
        "id": "vt3jZ8KkD3Xb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1:  Broadcastable tensors"
      ],
      "metadata": {
        "id": "8tcVJzRMFky0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorA = torch.randint(10 , size=[1,3])\n",
        "tensorB = torch.arange(3).reshape(3,1)\n",
        "\n",
        "print( tensorA , tensorA.size())\n",
        "print(tensorB , tensorB.size())\n",
        "\n",
        "tensorA1 , tensorB1 = my_broadcast_tensors(tensorA, tensorB)\n",
        "tensorA2 , tensorB2 = torch.broadcast_tensors(tensorA, tensorB)\n",
        "\n",
        "# Check if the tensors are equal\n",
        "if torch.equal(tensorA1, tensorA2) & torch.equal(tensorB1, tensorB2):\n",
        "    print(\"\\nmy_broadcast_tensors matches Torch's torch.broadcast_tensors:\\n\")\n",
        "    print( tensorA1 , tensorA1.size())\n",
        "    print(tensorB1 , tensorB1.size())\n",
        "else:\n",
        "    print(\"\\nmy_broadcast_tensors doesn't matches Torch's torch.broadcast_tensors:\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB7vLtpG0Liz",
        "outputId": "2b16bdfe-0323-43be-a042-7ee85676c222"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0]]) torch.Size([1, 3])\n",
            "tensor([[0],\n",
            "        [1],\n",
            "        [2]]) torch.Size([3, 1])\n",
            "\n",
            "my_broadcast_tensors matches Torch's torch.broadcast_tensors:\n",
            "\n",
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]]) torch.Size([3, 3])\n",
            "tensor([[0, 0, 0],\n",
            "        [1, 1, 1],\n",
            "        [2, 2, 2]]) torch.Size([3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2:  Broadcastable tensors"
      ],
      "metadata": {
        "id": "qgg9-6S91VVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorA = torch.randint(10 , size=[2,1,4,7])\n",
        "tensorB = torch.arange(5*2*3*1*7).reshape(5,2,3,1,7)\n",
        "\n",
        "tensorA1 , tensorB1 = my_broadcast_tensors(tensorA, tensorB)\n",
        "tensorA2 , tensorB2 = torch.broadcast_tensors(tensorA, tensorB)\n",
        "\n",
        "# Check if the tensors are equal\n",
        "if torch.equal(tensorA1, tensorA2) & torch.equal(tensorB1, tensorB2):\n",
        "    print(\"\\nmy_broadcast_tensors matches Torch's torch.broadcast_tensors\\n\")\n",
        "else:\n",
        "    print(\"\\nmy_broadcast_tensors doesn't matches Torch's torch.broadcast_tensors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI9gLY5k1VqL",
        "outputId": "57a6b011-6582-44a7-d82e-b354fec6d631"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "my_broadcast_tensors matches Torch's torch.broadcast_tensors\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 3: Unbroadcastable tensors example"
      ],
      "metadata": {
        "id": "9McxZ7hW1jwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorA = torch.randint(10 , size=[2,1,4,5])\n",
        "tensorB = torch.arange(5*2*3*1*7).reshape(5,2,3,1,7)\n",
        "\n",
        "print(\"my broadcast_tensors results:\")\n",
        "my_broadcast_tensors(tensorA, tensorB)\n",
        "\n",
        "print(\"\\ntorch.broadcast_tensors results:\")\n",
        "try:\n",
        "  torch.broadcast_tensors(tensorA, tensorB)\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPk9_4tH1p3o",
        "outputId": "d7b94fee-7a10-4194-c058-ca6f5c14f184"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my broadcast_tensors results:\n",
            "The size of tensor a (5) must match the size of tensor b (7) at non-singleton dimension 4\n",
            "\n",
            "torch.broadcast_tensors results:\n",
            "The size of tensor a (5) must match the size of tensor b (7) at non-singleton dimension 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 4: Unbroadcastable tensors example"
      ],
      "metadata": {
        "id": "XNf8QsQO3MCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorA = torch.randint(10 , size=[8,3,3,2,1,4,7])\n",
        "tensorB = torch.arange(5*2*3*4*7).reshape(5,2,3,4,7)\n",
        "\n",
        "print(\"my broadcast_tensors results:\")\n",
        "my_broadcast_tensors(tensorA, tensorB)\n",
        "\n",
        "print(\"\\ntorch.broadcast_tensors results:\")\n",
        "try:\n",
        "  torch.broadcast_tensors(tensorA, tensorB)\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7n-LUZy3LCa",
        "outputId": "b73338aa-a534-4353-960b-ba29fe258d55"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my broadcast_tensors results:\n",
            "The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 2\n",
            "\n",
            "torch.broadcast_tensors results:\n",
            "The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 2\n"
          ]
        }
      ]
    }
  ]
}