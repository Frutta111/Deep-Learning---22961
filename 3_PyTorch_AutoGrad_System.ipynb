{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu+63nnYk2hoXwWMjcpQ3R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frutta111/Deep-Learning-In-PyTorch/blob/main/3_PyTorch_AutoGrad_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Implementing PyTorch's Automatic Differentiation System**\n"
      ],
      "metadata": {
        "id": "salGr7D32MYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5/4/2024"
      ],
      "metadata": {
        "id": "S3sx_CQP4PsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will implement PyTorch's unique automatic differentiation system (Auto Grad) on several basic mathematical operations.\n",
        "\n",
        "Autograd is an automatic differentiation system that tracks operations performed on variables and computes the immediate derivative of the output with respect to the input. It then calculates the derivatives according to the chain rule.\n",
        "\n",
        "We will use three components in our implementation:\n",
        "\n",
        "1. A class called `MyScalar`.Each object of this class has the following attributes:\n",
        "    - **Scalar Value**: A numeric value. .\n",
        "    - **Immediate Derivative**: The derivative obtained during the computation of this object (also a numeric value).\n",
        "    - **Parent**: The input to the function represented by this object.\n",
        "\n",
        "2. **A library of mathematical functions** hat accept an object of the MyScalar class as input and return another object of the same class. The result of the functionâ€™s calculation will be stored in the value attribute, and the derivative with respect to the input variable will be stored in the derivative attribute. This library includes specific functions:  `e^a, ln(a), sin(a), cos(a), a^n, n * a, and a + n`. Each function calculates the derivative with respect to the variable and returns an output object of the `MyScalar` class.\n",
        "\n",
        "3. **A function named `get_gradient`** that returns a dictionary containing the derivative of a `MyScalar` object with respect to each of the variables it depends on.\n",
        "\n"
      ],
      "metadata": {
        "id": "wr_WEY9L2Tt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "jUUSky7g1aS0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let us define the class MyScalar,"
      ],
      "metadata": {
        "id": "3rTd0Yj_0Dkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyScalar:\n",
        "    \"\"\"\n",
        "    A class to represent a scalar value with automatic differentiation capabilities.\n",
        "    Each instance of this class has a unique identifier, a scalar value, an immediate derivative,\n",
        "    and references to its parent (input) object.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, value, derivative=1.0, parent = None):\n",
        "      \"\"\"\n",
        "        Initialize a MyScalar object.\n",
        "\n",
        "        Args:\n",
        "            value (float): The scalar value.\n",
        "            derivative (float, optional): The immediate derivative of the scalar. Defaults to 1.0.\n",
        "            parent (MyScalar, optional): The parent MyScalar object representing the input to the function. Defaults to None.\n",
        "      \"\"\"\n",
        "\n",
        "      if not torch.is_tensor(value):\n",
        "        value = torch.tensor(value , dtype=torch.float32)\n",
        "      if not torch.is_tensor(derivative):\n",
        "        derivative = torch.tensor(derivative,dtype=torch.float32)\n",
        "\n",
        "      self.value = value # The scalar value\n",
        "      self.derivative = derivative\n",
        "      self.parent = parent\n",
        "\n",
        "      if parent == None:\n",
        "        id_parent = None\n",
        "      else:\n",
        "        id_parent =id(self.parent)\n",
        "\n",
        "      print(f\"variable_id = {id(self)}, value = {self.value.item():.2f}, immediate_derivative = {self.derivative.item():.2f} , parent_id = {id_parent}\")"
      ],
      "metadata": {
        "id": "6KBAuDzS2Kvs"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we  will implement a library (methods of Myscalar) which contains several basic functions."
      ],
      "metadata": {
        "id": "qvmJVE0Q2Wqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a , n):\n",
        "    output = MyScalar(value = a.value + n , derivative=torch.tensor([1]),  parent = a)\n",
        "    return output\n",
        "\n",
        "def mult(a , n):\n",
        "    output = MyScalar(value = a.value*n , derivative=torch.tensor([n]), parent = a)\n",
        "    return output\n",
        "\n",
        "def power(a , n):\n",
        "    output = MyScalar(value = a.value**n , derivative=n*(a.value**(n-1)), parent = a)\n",
        "    return output\n",
        "\n",
        "def cos(a):\n",
        "    output = MyScalar(value = torch.cos(a.value) , derivative=-torch.sin(a.value), parent = a)\n",
        "    return output\n",
        "\n",
        "def sin(a):\n",
        "    output = MyScalar(value = torch.sin(a.value) , derivative=torch.cos(a.value), parent = a)\n",
        "    return output\n",
        "\n",
        "def ln(a):\n",
        "    output = MyScalar(value = torch.log(a.value) , derivative=a.value**(-1), parent = a)\n",
        "    return output\n",
        "\n",
        "def exp(a):\n",
        "    output = MyScalar(value = torch.exp(a.value) , derivative=torch.exp(a.value), parent = a)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "8MPxxIDI2XBk"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will preform the Gradient Calculation for MyScalar Objects using the chain rule.\n",
        "\n",
        "This code defines the:\n",
        "\n",
        "\n",
        "*  **`get_gradient`** function, which calculates the gradient of a MyScalar object with respect to its input variables.\n",
        "*  **`_get_gradient`** which is a helper function to recursively calculate the gradients and store them in a dictionary.\n",
        "*  The calculated gradients are then returned as **a dictionary**, where the keys are the unique identifiers of the MyScalar objects and the values are the corresponding gradients."
      ],
      "metadata": {
        "id": "BMS3diPK3GO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gradient(output):\n",
        "  \"\"\"\n",
        "  Calculate the gradient of the given MyScalar object with respect to its input variables.\n",
        "\n",
        "  Args:\n",
        "    output (MyScalar): The output MyScalar object for which to calculate the gradient.\n",
        "\n",
        "  Returns:\n",
        "      dict: A dictionary containing the gradients of the output with respect to each variable.\n",
        "  \"\"\"\n",
        "\n",
        "  def _get_gradient(output, gradients=None, gr=1):\n",
        "        \"\"\"\n",
        "        Recursive helper function to calculate gradients.\n",
        "\n",
        "        Args:\n",
        "            output (MyScalar): The current MyScalar object.\n",
        "            gradients (dict, optional): A dictionary to store gradients. Defaults to None.\n",
        "            gr (float, optional): The accumulated gradient. Defaults to 1.\n",
        "\n",
        "        Returns:\n",
        "            dict: Updated dictionary containing the gradients.\n",
        "        \"\"\"\n",
        "\n",
        "        if gradients is None:\n",
        "            gradients = {}\n",
        "\n",
        "        # If the current object has no parent, it is a variable, so store the gradient\n",
        "        if output.parent is None:\n",
        "            gradients[id(output)] =  gr\n",
        "            return gradients\n",
        "\n",
        "        # Accumulate the gradient\n",
        "        gr = gr * output.derivative\n",
        "        _get_gradient(output.parent, gradients, gr)\n",
        "\n",
        "        # Store the gradient for the current object\n",
        "        gradients[id(output)] =  gr / output.derivative\n",
        "\n",
        "        return gradients\n",
        "\n",
        "  # Start the gradient calculation from the output object\n",
        "  return _get_gradient(output)"
      ],
      "metadata": {
        "id": "pJIIglWm3Gn_"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function prints the gradients of MyScalar objects"
      ],
      "metadata": {
        "id": "E7IHrh6e6fKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_gradient(gradients):\n",
        "  for id, value   in gradients.items():\n",
        "    print(f\"variable gradient w.r.t parent_id = {id}, gradient value = {value.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "6GPV51Ve6bpX"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usage Example\n",
        "In the following examples we will create variables from the `MyScalar` class and run the derivation system `get_gradient` on it. For comparison we will define tensors and run PyTorch's automatic derivation system. We will use the [`retain_grad`](https://pytorch.org/docs/stable/generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad) function that allows you to keep the partial derivatives during the calculations."
      ],
      "metadata": {
        "id": "4KWLuuCVLEBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Exapmle 1\n",
        "\n",
        "We will creating a scalar with value=2 and preform several mathematical operators to define new variabels"
      ],
      "metadata": {
        "id": "Pw56wovW4nia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the gradients using MyScalar System:\n",
        "a = MyScalar(torch.tensor([2]))\n",
        "b = power(a,2) #a^2\n",
        "c = exp(b)  #e^a\n",
        "\n",
        "print(\"\\nThe gradients using my_gradient function are:\")\n",
        "grad = get_gradient(c)\n",
        "print_gradient(grad)\n",
        "\n",
        "# Print the gradients using Aoutograd System:\n",
        "a = torch.tensor([2.], requires_grad=True)\n",
        "b = a ** 2\n",
        "c = torch.exp(b)\n",
        "\n",
        "b.retain_grad()\n",
        "c.retain_grad()\n",
        "\n",
        "c.backward()\n",
        "\n",
        "print(\"\\nthe gradients using aoutograd are:\")\n",
        "print(\"dc/da =\", a.grad.item())  # Gradient of 'c' w.r.t 'a'\n",
        "print(\"dc/db =\", b.grad.item())  # Gradient of 'c' w.r.t 'b'\n",
        "print(\"dc/dc =\", c.grad.item())  # Gradient of 'c' w.r.t 'c'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLdZHfRsAKEh",
        "outputId": "0a7f633b-e8a6-4bcf-ec3b-5da24628aeee"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "variable_id = 138670482557008, value = 2.00, immediate_derivative = 1.00 , parent_id = None\n",
            "variable_id = 138670482569344, value = 4.00, immediate_derivative = 4.00 , parent_id = 138670482557008\n",
            "variable_id = 138670482567424, value = 54.60, immediate_derivative = 54.60 , parent_id = 138670482569344\n",
            "\n",
            "The gradients using my_gradient function are:\n",
            "variable gradient w.r.t parent_id = 138670482557008, gradient value = 218.3926\n",
            "variable gradient w.r.t parent_id = 138670482569344, gradient value = 54.5981\n",
            "variable gradient w.r.t parent_id = 138670482567424, gradient value = 1.0000\n",
            "\n",
            "the gradients using aoutograd are:\n",
            "dc/da = 218.39259338378906\n",
            "dc/db = 54.598148345947266\n",
            "dc/dc = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Example 2\n",
        "\n",
        "We will creating a scalar with value=2 and preform several mathematical operators to define new variabels"
      ],
      "metadata": {
        "id": "RuDu_nFX8TWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the gradients using MyScalar System:\n",
        "a = MyScalar(2)\n",
        "b = mult(a,3)\n",
        "c = mult(b,7)\n",
        "d = add(c,10)\n",
        "e = mult(d,6)\n",
        "\n",
        "print(\"\\nThe gradients using my_gradient function are:\")\n",
        "grad = get_gradient(e)\n",
        "print_gradient(grad)\n",
        "\n",
        "# Print the gradients using Aoutograd System:\n",
        "a = torch.tensor([2.], requires_grad=True)\n",
        "b = a * 3\n",
        "c = b * 7\n",
        "d = c + 10\n",
        "e = d * 6\n",
        "\n",
        "b.retain_grad()\n",
        "c.retain_grad()\n",
        "d.retain_grad()\n",
        "e.retain_grad()\n",
        "\n",
        "e.backward()\n",
        "\n",
        "print(\"\\nThe gradients using aoutograd are:\")\n",
        "print(\"de/da =\", a.grad.item())  # Gradient of 'e' w.r.t 'a'\n",
        "print(\"de/db =\", b.grad.item())  # Gradient of 'e' w.r.t 'b'\n",
        "print(\"de/dc =\", c.grad.item())  # Gradient of 'e' w.r.t 'c'\n",
        "print(\"de/dd =\", d.grad.item())  # Gradient of 'e' w.r.t 'd'\n",
        "print(\"de/de =\", e.grad.item())  # Gradient of 'e' w.r.t 'e'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6atYTj_8Nyl",
        "outputId": "698ab0c6-daaa-4dd1-931f-91f2867031df"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "variable_id = 138670482562720, value = 2.00, immediate_derivative = 1.00 , parent_id = None\n",
            "variable_id = 138670482564064, value = 6.00, immediate_derivative = 3.00 , parent_id = 138670482562720\n",
            "variable_id = 138670482558736, value = 42.00, immediate_derivative = 7.00 , parent_id = 138670482564064\n",
            "variable_id = 138670482559120, value = 52.00, immediate_derivative = 1.00 , parent_id = 138670482558736\n",
            "variable_id = 138670482570736, value = 312.00, immediate_derivative = 6.00 , parent_id = 138670482559120\n",
            "\n",
            "The gradients using my_gradient function are:\n",
            "variable gradient w.r.t parent_id = 138670482562720, gradient value = 126.0000\n",
            "variable gradient w.r.t parent_id = 138670482564064, gradient value = 42.0000\n",
            "variable gradient w.r.t parent_id = 138670482558736, gradient value = 6.0000\n",
            "variable gradient w.r.t parent_id = 138670482559120, gradient value = 6.0000\n",
            "variable gradient w.r.t parent_id = 138670482570736, gradient value = 1.0000\n",
            "\n",
            "The gradients using aoutograd are:\n",
            "de/da = 126.0\n",
            "de/db = 42.0\n",
            "de/dc = 6.0\n",
            "de/dd = 6.0\n",
            "de/de = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Example 3\n",
        "\n",
        "We will creating a scalar with value=2 and preform several mathematical operators to define new variabels"
      ],
      "metadata": {
        "id": "M3PzHmZT8ZJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the gradients using MyScalar System:\n",
        "a = MyScalar(torch.tensor([2]))\n",
        "b = power(a,2) #a^2\n",
        "c = exp(b)\n",
        "d = sin(c)\n",
        "e = cos(d)\n",
        "f = ln(e)\n",
        "\n",
        "print(\"\\nThe gradients using my_gradient function are:\")\n",
        "grad = get_gradient(f)\n",
        "print_gradient(grad)\n",
        "\n",
        "\n",
        "# Print the gradients using Aoutograd System:\n",
        "a = torch.tensor([2.], requires_grad=True)\n",
        "b = a ** 2 #a^2\n",
        "c = torch.exp(b)\n",
        "d = torch.sin(c)\n",
        "e = torch.cos(d)\n",
        "f = torch.log(e)\n",
        "\n",
        "\n",
        "b.retain_grad()\n",
        "c.retain_grad()\n",
        "d.retain_grad()\n",
        "e.retain_grad()\n",
        "f.retain_grad()\n",
        "\n",
        "f.backward()\n",
        "\n",
        "print(\"\\nthe gradients using aoutograd are:\")\n",
        "print(\"df/da =\", a.grad.item())  # Gradient of 'f' w.r.t 'a'\n",
        "print(\"df/db =\", b.grad.item())  # Gradient of 'f' w.r.t 'b'\n",
        "print(\"df/dc =\", c.grad.item())  # Gradient of 'f' w.r.t 'c'\n",
        "print(\"df/dd =\", d.grad.item())  # Gradient of 'f' w.r.t 'd'\n",
        "print(\"df/de =\", e.grad.item())  # Gradient of 'f' w.r.t 'e'\n",
        "print(\"df/df =\", f.grad.item())  # Gradient of 'f' w.r.t 'f'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YqBg1KX8bs5",
        "outputId": "74e4e73d-df46-455c-cb07-ea9fdabead29"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "variable_id = 138670481915280, value = 2.00, immediate_derivative = 1.00 , parent_id = None\n",
            "variable_id = 138670482565264, value = 4.00, immediate_derivative = 4.00 , parent_id = 138670481915280\n",
            "variable_id = 138670482558400, value = 54.60, immediate_derivative = 54.60 , parent_id = 138670482565264\n",
            "variable_id = 138670482570736, value = -0.93, immediate_derivative = -0.37 , parent_id = 138670482558400\n",
            "variable_id = 138670482566032, value = 0.60, immediate_derivative = 0.80 , parent_id = 138670482570736\n",
            "variable_id = 138670482564640, value = -0.51, immediate_derivative = 1.67 , parent_id = 138670482566032\n",
            "\n",
            "The gradients using my_gradient function are:\n",
            "variable gradient w.r.t parent_id = 138670481915280, gradient value = -108.2652\n",
            "variable gradient w.r.t parent_id = 138670482565264, gradient value = -27.0663\n",
            "variable gradient w.r.t parent_id = 138670482558400, gradient value = -0.4957\n",
            "variable gradient w.r.t parent_id = 138670482570736, gradient value = 1.3374\n",
            "variable gradient w.r.t parent_id = 138670482566032, gradient value = 1.6699\n",
            "variable gradient w.r.t parent_id = 138670482564640, gradient value = 1.0000\n",
            "\n",
            "the gradients using aoutograd are:\n",
            "df/da = -108.26516723632812\n",
            "df/db = -27.06629180908203\n",
            "df/dc = -0.4957364499568939\n",
            "df/dd = 1.3374305963516235\n",
            "df/de = 1.669946312904358\n",
            "df/df = 1.0\n"
          ]
        }
      ]
    }
  ]
}